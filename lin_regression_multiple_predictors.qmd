# Linear regression notation for models with multiple predictors

```{python}
# Import numerical and plotting libraries
import numpy as np
import numpy.linalg as npl
import matplotlib.pyplot as plt
import pandas as pd

# For displaying variables in mathematical form.
from jupyprint import jupyprint, arraytex

# For interactive widgets.
from ipywidgets import interact
```

```{python}
# read in the data
df = pd.read_csv("data/Duncan_Occupational_Prestige.csv")

# restrict our attention to the first 15 rows
df = df.head(15)

# show the data
df
```

```{python}
education = np.array(df['education'])
prestige = np.array(df['prestige'])
income = np.array(df['income'])
```

```{python}
def interactive_notation(edu_slope_guess=1,
                         inc_slope_guess=1,
                         intercept_guess=1):

    # calculate the fitted values, for this combination of parameter estimates
    fitted = edu_slope_guess * education + inc_slope_guess * income + intercept_guess

    # calculate the errors, for this combination of parameter estimates
    errors = prestige - fitted

    # do not worry about this code, it just prints the mathematical notation below this cell
    jupyprint("$\\vec{\\hat{y}} = b_1 * $ `education` + $b_2 * $ `income` +   $\\text{c} $")
    jupyprint(f"${arraytex(np.atleast_2d(fitted + errors).astype(int).T)} = {round(edu_slope_guess, 2)} * {arraytex(np.atleast_2d(education).T)} + {round(inc_slope_guess, 2)} * {arraytex(np.atleast_2d(income).T)} + {round(intercept_guess, 2)} + {arraytex(np.atleast_2d(errors).round(2).T)}$")
    jupyprint(f"The sum of the squared errors for this combination of parameter estimates is <b> {round(np.sum(errors**2), 2)} </b>")

interact(interactive_notation, edu_slope_guess = (-1, 1, 0.1), inc_slope_guess = (-1, 1, 0.1), intercept_guess = (-10, 10, 0.1))
```
