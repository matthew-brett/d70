# Linear regression notation for models with multiple predictors

This page will introduce the notation for linear regression models with
multiple predictor variables, expanding on the notation which we have already
seen for [linear regression models with a single
predictor](lin_regression_notation).
 
We will add the same caveat as before: this page will most likely be
challenging if you are new to this, so please do not worry if you do not
understand some concepts the first time you read them.

But do keep reading, and re-reading.  You'll get there.

```{python}
# Import numerical and plotting libraries
import numpy as np
import numpy.linalg as npl
import matplotlib.pyplot as plt
import pandas as pd

# For displaying variables in mathematical form.
from jupyprint import jupyprint, arraytex

# Only show 6 decimals when printing arrays.
np.set_printoptions(precision=6)

# The Scipy statistics submodule, for linregress
import scipy.stats as sps

# For interactive widgets.
from ipywidgets import interact
```

We will again use the Duncan (1961) occupational prestige dataset. The dataset
combines information from the 1950 U.S. Census with data collected by the
National Opinion Research Centre (NORC). The Census data contained information
about different occupations, such as the percentage of people working in that
occupation who earned over a certain amount per year. The NORC data was from
a survey which asked participants to rate how prestigious they considered each
occupation. Here are the descriptions of the variables in the dataset, which
covers 45 occupations (adapted from
[here](https://rdrr.io/cran/carData/man/Duncan.html)):

* `name` - the name of the occupation, from the 1950 US Census
* `type`- type of occupation, with the following categories ``prof``,
  professional and managerial; ``wc``, white-collar; ``bc``, blue-collar. (E.g.
  how the occupation was classified in the 1950 US Census)
* `income` - percentage of census respondents within the occupation who earned
  3,500 dollars or more per year (about 36,000 US dollars in 2017)
* `education` - percentage of census respondents within the occupation who were
  high school graduates 
* `prestige` - percentage of respondents in the NORC survey who rated the
  occupation as “good” or better in prestige

See [the dataset
page](https://github.com/odsti/datasets/tree/main/duncan_occupations) for more
detail.

Again, we will restrict our attention to the first 15 observations in the
dataset (to make it easier to visualise/think about the concepts):

```{python}
# read in the data
df = pd.read_csv("data/Duncan_Occupational_Prestige.csv")

# restrict our attention to the first 15 rows
df = df.head(15)

# show the data
df
```

The observational units in this dataset are professions, and you'll remember from the previous page, that the linear regression model we fit was as follows:

`prestige` = $b * $ `education` + $ \text{c} + \vec{\varepsilon} $

The model equation reads as "we are modeling `prestige` as a linear function of
`education`, plus `error`".

The `error` is the randomness that remains after the straight line relationship
between `education` and `prestige` is accounted for e.g. the y-axis distance
between each data point and the line of best fit.

Remember that `prestige` and `education` are vectors of 15 values.

Just for ease of typing, let's store the vectors again as separate python
variables.

```{python}
# store the education values as a variable
education = np.array(df['education'])

# show the values
education
```

```{python}
# store the prestige values as a variable 
prestige = np.array(df['prestige'])

# show the values
prestige
```

```{python}
# Remember we might also want to refer to these vectors (in python) by the
# names we will use for mathematical notation.
x = education
y = prestige
```

We learned the mathematical notation for the model we fit previously:

$ \vec{y} = b \vec{x} + \text{c} + \vec{\varepsilon} $

$\begin{bmatrix}{} \text{$y_{1}$} \\ \text{$y_{2}$} \\ \text{$y_{3}$} \\ \text{$y_{4}$} \\ \text{$y_{5}$} \\ \text{$y_{6}$} \\ \text{$y_{7}$} \\ \text{$y_{8}$} \\ \text{$y_{9}$} \\ \text{$y_{10}$} \\ \text{$y_{11}$} \\ \text{$y_{12}$} \\ \text{$y_{13}$} \\ \text{$y_{14}$} \\ \text{$y_{15}$} \\ \end{bmatrix} = b * \begin{bmatrix}{} \text{$x_{1}$} \\ \text{$x_{2}$} \\ \text{$x_{3}$} \\ \text{$x_{4}$} \\ \text{$x_{5}$} \\ \text{$x_{6}$} \\ \text{$x_{7}$} \\ \text{$x_{8}$} \\ \text{$x_{9}$} \\ \text{$x_{10}$} \\ \text{$x_{11}$} \\ \text{$x_{12}$} \\ \text{$x_{13}$} \\ \text{$x_{14}$} \\ \text{$x_{15}$} \\ \end{bmatrix} + c + \begin{bmatrix}{} \text{$\varepsilon_{1}$} \\ \text{$\varepsilon_{2}$} \\ \text{$\varepsilon_{3}$} \\ \text{$\varepsilon_{4}$} \\ \text{$\varepsilon_{5}$} \\ \text{$\varepsilon_{6}$} \\ \text{$\varepsilon_{7}$} \\ \text{$\varepsilon_{8}$} \\ \text{$\varepsilon_{9}$} \\ \text{$\varepsilon_{10}$} \\ \text{$\varepsilon_{11}$} \\ \text{$\varepsilon_{12}$} \\ \text{$\varepsilon_{13}$} \\ \text{$\varepsilon_{14}$} \\ \text{$\varepsilon_{15}$} \\ \end{bmatrix}$

Run the cell below which will remind us how the model looks with the data from our actual `prestige` and `education` vectors:

```{python}
# do not worry about this code, it just prints the mathematical notation below this cell
jupyprint("Here is the our model ($ \\vec{y} = b \\vec{x} + \\text{c} + \\vec{\\varepsilon} $), showing the actual values within the `education` and `prestige` vectors:")
jupyprint(f"${arraytex(np.atleast_2d(prestige).T)} = b * {arraytex(np.atleast_2d(education).T)} + c +" +" \\begin{bmatrix}{} \\text{$\\varepsilon_{1}$} \\\\ \\text{$\\varepsilon_{2}$} \\\\ \\text{$\\varepsilon_{3}$} \\\\ \\text{$\\varepsilon_{4}$} \\\\ \\text{$\\varepsilon_{5}$} \\\\ \\text{$\\varepsilon_{6}$} \\\\ \\text{$\\varepsilon_{7}$} \\\\ \\text{$\\varepsilon_{8}$} \\\\ \\text{$\\varepsilon_{9}$} \\\\ \\text{$\\varepsilon_{10}$} \\\\ \\text{$\\varepsilon_{11}$} \\\\ \\text{$\\varepsilon_{12}$} \\\\ \\text{$\\varepsilon_{13}$} \\\\ \\text{$\\varepsilon_{14}$} \\\\ \\text{$\\varepsilon_{15}$} \\\\ \\end{bmatrix}$")
```

Just to remind ourselves about the parameter estimates $b$ and $c$ of the
best-fitting line, let's fit the model again.

As we will be fitting several different models on this page, we will store the
slope and intercept using python variable names which indicate which model they
are from. In this case we will use `b_education` and `c_education` to indicate
that they are the slope and intercept from a linear regression using
`education` as the predictor variable:

*Note*: `prestige` will always be the outcome variable, for all models fit on
this page.

```{python}
# fit a linear regression model, using scipy
res = sps.linregress(education, prestige)

# store the slope (b) and the intercept (c) as separate variables
b_education = res.slope
c_education = res.intercept

# show the slope and intercept
jupyprint("For the model (`prestige` = $b * $ `education` + $ \\text{c} + \\vec{\\varepsilon}) $,"+f" the slope is {b_education.round(2)} and the intercept is {c_education.round(2)}.")
```

Here are the data, and the fitted values from the model, shown graphically:

```{python}
# do not worry about this code, it is just a convenience function to plot the data
def make_scatter(with_errors=False,
                 show=False,
                 return_errors=False,
                 b=b_education,
                 c=c_education,
                 x=education,
                 y=prestige,
                 xlabel='Education',
                 ylabel='Prestige',
                 legend_loc=(1, 0.6),
                 continuous_line=False):
    plt.scatter(x, y, label='Actual values ($y$)')
    # plot the predicted values
    fitted = b * x + c
    if continuous_line == False:
        plt.plot(x, fitted, 'ro', label='Fitted values from linear regression ($\hat{y}$)')
    elif continuous_line == True:
        x_for_plot = np.linspace(x.min(), x.max())
        fitted_for_plot = b * x_for_plot + c
        plt.plot(x_for_plot, fitted_for_plot, 'ro', label='Edge of linear regression plane ($\hat{y}$)')
    if with_errors == True:
        # plot the distance between predicted and actual, for all points.
        n = len(x)
        for i in range(n):
            plt.plot([x[i], x[i]], [fitted[i], y[i]], 'k:')
        # the following code line is just to trick Matplotlib into making a new
        # a single legend entry for the dotted lines.
        plt.plot([], [], 'k:', label='Errors ($ \\varepsilon $)')
    plt.xlabel(xlabel)
    plt.ylabel(ylabel)
    plt.title(f"\n$b$ = {round(b,2)} \n$c$ = {round(c,2)} \n Sum of Squared Error = {round(np.sum((prestige - (b*x + c))**2), 2)}")
    # show the legend
    plt.legend(loc = legend_loc);
    if show == True:
        plt.show()
    # return and show the error vector?
    if return_errors == True:
        errors = y - fitted
        jupyprint(f"Here is the error vector for the current line: {arraytex(np.atleast_2d(errors.round(2)).T)}")
        jupyprint(f"The sum of the squared error is <b> {round(np.sum((errors)**2),2)}. </b>")
        return errors 

# some other convenience functions
def make_scatter_line_comparison(b, c):
    # Call the make_scatter function with some defaults.
    errors = make_scatter(with_errors = True,
                          return_errors = True,
                          b = b,
                          c = c,
                          show = True)
    return errors

# generate the plot.
make_scatter(with_errors=True)
```

## Linear regression using `scipy.optimize`

In order to make sense of the notation for having multiple predictors in the
model, we need to pay attention to the *cost function* used in linear
regression. The method of fitting regression models we used above, and on the
previous page, involves a pre-written function (`scipy.stats.linregress`) which
uses its own cost function.

Because the `scipy` library is open source, we can look at the cost function it
uses. But, to deepen our understanding of the notation for linear regression
with multiple predictors, let's re-visit another method of fitting linear
regression models, which you've already seen on a [previous
page](https://lisds.github.io/textbook/mean-slopes/finding_lines.html).

Here we write our own *cost function*.  The *cost function* for a line is
a function that returns a quality-of-fit value for a particular line.  That is,
it accepts parameters of the slope and intercept defining the line, and returns
a value that will be *low* (low cost) for a well-fitting line, and *high* (high
cost) for a poorly fitting line.

We have seen different cost functions for linear regression before [on this
page](https://lisds.github.io/textbook/mean-slopes/sse_rmse.html). You'll
recall that we can use the *sum of the squared* errors or the *root mean
squared error* - they will identify the same slope and intercept as giving the
lowest cost. As we are working with a small number of observations, let's use
the sum of the squared errors.

The cell below defines a function which takes a sequence (`b_and_c`) containing
a slope and an intercept value. It then calculates fitted values for that slope
and intercept pair, using the formula we saw on the [linear regression notation
page](https://nbviewer.org/github/pxr687/temp_page/blob/main/lin_regression_notation.ipynb)

```{python}
def ss_any_line(b_and_c, x_vector, y_vector):
    """ Sum of squares error for slope, intercept `b_and_c`
    """
    # unpack the list containing the slope and the intercept
    b, c = b_and_c

    # calculate the fitted values, for this slope/intercept pairing
    fitted = b*x_vector + c

    # calculate the error vector
    error = y_vector - fitted

    # return the value of the cost function (the cost).
    return np.sum(error ** 2)
```

Let's see what the sum of squared error is for an arbitrarily chosen slope and
intercept value.

```{python}
# arbitrary slope and intercept pair
arbitrary_slope_intercept_pair = [10, 10]

# Calculating the sum of squared error using our function
ss_any_line(arbitrary_slope_intercept_pair, education, prestige)
```

The graph below shows the line that results from this slope and intercept pair,
along with the original data (blue dots) and the errors (black dashed lines).

Remember the mathematical form of a linear regression model with a single
predictor:

$ \vec{y} = b \vec{x} + \text{c} + \vec{\varepsilon} $

$\begin{bmatrix}{} \text{$y_{1}$} \\ \text{$y_{2}$} \\ \text{$y_{3}$} \\ \text{$y_{4}$} \\ \text{$y_{5}$} \\ \text{$y_{6}$} \\ \text{$y_{7}$} \\ \text{$y_{8}$} \\ \text{$y_{9}$} \\ \text{$y_{10}$} \\ \text{$y_{11}$} \\ \text{$y_{12}$} \\ \text{$y_{13}$} \\ \text{$y_{14}$} \\ \text{$y_{15}$} \\ \end{bmatrix} = b * \begin{bmatrix}{} \text{$x_{1}$} \\ \text{$x_{2}$} \\ \text{$x_{3}$} \\ \text{$x_{4}$} \\ \text{$x_{5}$} \\ \text{$x_{6}$} \\ \text{$x_{7}$} \\ \text{$x_{8}$} \\ \text{$x_{9}$} \\ \text{$x_{10}$} \\ \text{$x_{11}$} \\ \text{$x_{12}$} \\ \text{$x_{13}$} \\ \text{$x_{14}$} \\ \text{$x_{15}$} \\ \end{bmatrix} + c + \begin{bmatrix}{} \text{$\varepsilon_{1}$} \\ \text{$\varepsilon_{2}$} \\ \text{$\varepsilon_{3}$} \\ \text{$\varepsilon_{4}$} \\ \text{$\varepsilon_{5}$} \\ \text{$\varepsilon_{6}$} \\ \text{$\varepsilon_{7}$} \\ \text{$\varepsilon_{8}$} \\ \text{$\varepsilon_{9}$} \\ \text{$\varepsilon_{10}$} \\ \text{$\varepsilon_{11}$} \\ \text{$\varepsilon_{12}$} \\ \text{$\varepsilon_{13}$} \\ \text{$\varepsilon_{14}$} \\ \text{$\varepsilon_{15}$} \\ \end{bmatrix}$

Let's make sure we can relate all aspects of the notation to the graph below
(the error vector for this slope/intercept pair, as well as the sum of the
squared error is shown below the graph):

*Note*: if you are using this notebook interactively, try and use a few
different slope and intercept pairs by changing the value of `b` and `c` in the
input to the `make_scatter` function in the cell below:

```{python}
# tyr some different arbitrary slope and intercept pairs
arbitrary_slope_intercept_pair = [10, 10]

# do not worry about this code, it just generates the plot/printouts
errors_from_guess = make_scatter_line_comparison(b = arbitrary_slope_intercept_pair[0],
                                                 c = arbitrary_slope_intercept_pair[1])
```

Let's compare this to the graph, the error vector and the sum of the squared
error value from our arbitrarily guessed pairing to those from the parameter
estimates for the best-fitting line:

```{python}
# the best-fitting line
errors_education = make_scatter_line_comparison(b = b_education, c = c_education)
```

We can see that when we compare the badly fitting line with the best-fitting
line the value of the cost function (the sum of squared error, calculated via
our `ss_any_line` function) is smaller for the best fitting line.

Just now, and on the [regression notation
page](https://nbviewer.org/github/pxr687/temp_page/blob/main/lin_regression_notation.ipynb)
we used the `scipy.stats.linregress` function to quickly fit our regression.
This essentially fits the model using a cost function that is internal to
`scipy.stats.linregress` — we do not get to control the cost function.

Let's refresh our memories on how to find the parameter estimates using a cost
function which we have written. We do this by passing our cost function
(`ss_any_line`) to the `minimize` function from the `scipy.optimize`.
`scipy.optimize` uses an efficient method to try a variety of slope/intercept
pairings until it finds the pair which give the minimum value of the cost
function:

```{python}
# import minimize
from scipy.optimize import minimize

# An initial guess at the slope/intercept pair, for `minimize` to start with.
initial_guess = [1, 1]

# Minimizing the sum of squares error
min_ss = minimize(ss_any_line, # the function we are minimizing
                  initial_guess, # a list of initial guesses for the parameters
                  args=(education, prestige)) # other arguments to give to the `minimize` function
min_ss
```
